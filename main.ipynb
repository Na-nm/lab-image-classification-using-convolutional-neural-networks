{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrb20KGMtTFq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJO8tVPhVPU7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(\"Labels: \", np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LFD0C0sVPU8"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "\n",
        "# Plot 10 random samples from each class\n",
        "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
        "fig.suptitle(\"10 Random Samples from Each CIFAR-10 Class\", fontsize=14)\n",
        "\n",
        "for class_idx in range(10):\n",
        "    class_images = x_train[y_train.flatten() == class_idx]  # Filter images by class\n",
        "    sample_images = class_images[np.random.choice(class_images.shape[0], 10, replace=False)]  # Randomly pick 10\n",
        "\n",
        "    for j in range(10): # Display the samples\n",
        "        axes[class_idx, j].imshow(sample_images[j])\n",
        "        axes[class_idx, j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTooZ3HrVPU9",
        "outputId": "27e35162-a11b-4518-9ed3-f4f92b25a0ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Normalize images (convert pixel values from [0, 255] to [0, 1])\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "print(f\"y_train shape: {y_train_one_hot.shape}\")\n",
        "print(f\"y_test shape: {y_test_one_hot.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "iSN6riPISBMG",
        "outputId": "666bcf1c-1bb3-4cff-fedc-472c0bdb8f66"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (32, 32, 3)\n",
        "num_classes = 10\n",
        "\n",
        "# Build the CNN model\n",
        "model1 = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "\n",
        "    # Max pooling layer\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flattening the feature maps\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected layer with 100 units\n",
        "    layers.Dense(100, activation=\"relu\"),\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model1.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hn8UzPBZugVp",
        "outputId": "e6424379-c4ed-4a5e-f1a9-f8dc25d5b73d"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Compile the model\n",
        "model1.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.SGD(),  # Stochastic Gradient Descent\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history1 = model1.fit(\n",
        "    x_train, y_train_one_hot,\n",
        "    validation_data=(x_test, y_test_one_hot),\n",
        "    epochs=50,\n",
        "    batch_size=512,\n",
        "    validation_split=0.1,  # 10% of training data used for validation\n",
        "    verbose=1 #Shows a progress bar for each epoch. - Displays loss and accuracy after each epoch.\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model1.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAsIz2TAVPVD"
      },
      "source": [
        "## Save the trained model then Load it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B_ZV6yJVPVD",
        "outputId": "23f9f96b-7f88-4531-b0b9-2a85f2201179"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model1.save('cifar10_model.h5')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('cifar10_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8WFDgu3VPVD"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "aFx27CTXVPVE",
        "outputId": "7533453c-2e68-455a-f37a-c0ed0a8456db"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Extract loss and accuracy values\n",
        "train_loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "train_acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "\n",
        "# Plot Loss Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)  # Create subplot (1 row, 2 cols, 1st plot)\n",
        "plt.plot(train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(val_loss, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy Curve\n",
        "plt.subplot(1, 2, 2)  # Create subplot (1 row, 2 cols, 2nd plot)\n",
        "plt.plot(train_acc, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_acc, label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Show plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgca5dUNSFNc",
        "outputId": "4e54d294-0618-434b-81af-4d0bb8d8e8b5"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Define the model\n",
        "model2 = models.Sequential()\n",
        "\n",
        "# First convolutional block with 32 filters, 3x3 kernel, ReLU activation, and same padding\n",
        "model2.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model2.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# Max pooling layer\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output of the previous layer\n",
        "model2.add(layers.Flatten())\n",
        "\n",
        "# Dense layer with 128 units and ReLU activation\n",
        "model2.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer with 10 units (one for each class) and softmax activation\n",
        "model2.add(layers.Dense(10, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "ed900cc4-3cf2-48ae-930e-9e248ae50e5b"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Compile the model with SGD optimizer and categorical_crossentropy loss\n",
        "model2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model2.summary()\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Normalize the images to the range [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Train the model for 50 epochs with a batch size of 512\n",
        "history2 = model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "F8OSHAf5SJPr",
        "outputId": "371e60ae-cab5-4ae1-a9b5-dee33724ab34"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# Plot Loss Curves\n",
        "axes[0].plot(history1.history['loss'], label='Model 1 Train')\n",
        "axes[0].plot(history1.history['val_loss'], label='Model 1 Validation')\n",
        "axes[0].plot(history2.history['loss'], label='VGG Model Train')\n",
        "axes[0].plot(history2.history['val_loss'], label='VGG Model Validation')\n",
        "axes[0].set_title('Loss Comparison')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "axes[1].plot(history1.history['accuracy'], label='Model 1 Train')\n",
        "axes[1].plot(history1.history['val_accuracy'], label='Model 1 Validation')\n",
        "axes[1].plot(history2.history['accuracy'], label='VGG Model Train')\n",
        "axes[1].plot(history2.history['val_accuracy'], label='VGG Model Validation')\n",
        "axes[1].set_title('Accuracy Comparison')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Comment on the observation**\n",
        "\n",
        "*( Deeper Model Performance: The deeper model with the SGD optimizer may take longer to converge, but it could perform similarly or better in terms of generalization (validation accuracy) after some epochs.\n",
        ")*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DObaoxhaSMUg"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Predict using model1 and model2\n",
        "y_pred_model1 = model1.predict(x_test)\n",
        "y_pred_model2 = model2.predict(x_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels (argmax)\n",
        "y_pred_model1 = np.argmax(y_pred_model1, axis=1)\n",
        "y_pred_model2 = np.argmax(y_pred_model2, axis=1)\n",
        "\n",
        "# Predict the results with the model\n",
        "new_pred = model2.predict(x_test)\n",
        "\n",
        "# Convert predictions to the class with the highest probability\n",
        "new_pred = np.argmax(new_pred, axis=1)\n",
        "\n",
        "# Convert ground truth labels to a 1D array (class indices)\n",
        "gt = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(gt, new_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for VGG Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "f778d459-bb30-4df7-f045-2773a5a41fe5"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Evaluate model1 on test data\n",
        "test_loss1, test_acc1 = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy for Model 1: {test_acc1:.4f}\")\n",
        "\n",
        "# Evaluate model2 on test data\n",
        "test_loss2, test_acc2 = model2.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy for Model VGG: {test_acc2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "5ff80573-a93f-4def-c429-2252fbfd085f"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 1. Resize images from (32, 32, 3) to (64, 64, 3)\n",
        "X_train = tf.image.resize(x_train, (64, 64))\n",
        "X_test = tf.image.resize(x_test, (64, 64))\n",
        "\n",
        "# 2. Convert the labels to one-hot encoded form\n",
        "Y_train = to_categorical(y_train, num_classes=10)\n",
        "Y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 3. Normalize the images\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "\n",
        "# Define the VGG-style CNN model\n",
        "model3 = models.Sequential([\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3), padding='same'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten the output to convert 2D feature maps into a 1D feature vector\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected dense layer with 128 neurons\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer: 10 neurons (one for each class) with softmax activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elnDWnjEbmO"
      },
      "outputs": [],
      "source": [
        "# Train the model for 50 epochs with batch size 512\n",
        "model3.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history3 = model3.fit(x_train, y_train, batch_size=512, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Predict on the test set\n",
        "new_VGG = model3.predict(x_test)\n",
        "\n",
        "# Get the class predictions (highest probability class)\n",
        "y_pred_classes = np.argmax(new_VGG, axis=1)\n",
        "\n",
        "# Get the true labels (original labels for the test set)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f'{i+1}' for i in range(10)], yticklabels=[f'{i+1}' for i in range(10)])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=[str(i) for i in range(1, 11)])\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for each class:\\n\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "\n",
        "1 - Use of activation functions:\n",
        "\n",
        "\n",
        "\n",
        "_ Activation functions help neural networks learn complex patterns by introducing non-linearity, enabling them to model real-world data effectively.\n",
        "\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "\n",
        "\n",
        "\n",
        "_ Sigmoid vs. Softmax: Sigmoid outputs a probability for a single neuron (used for binary classification), while Softmax assigns probabilities across multiple classes (used for multi-class classification).\n",
        "\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        "\n",
        "\n",
        "_ Categorical vs. Binary Crossentropy: Categorical Crossentropy is used for multi-class classification (with one-hot encoded labels), while Binary Crossentropy is for binary classification (labels as 0 or 1).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
